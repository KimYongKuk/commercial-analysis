"""
RAG (Retrieval-Augmented Generation) íŒŒì´í”„ë¼ì¸

ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ LLMì´ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
"""

from typing import List, Dict, Any, Optional
from openai import OpenAI
import os
from .retriever import Retriever
from .embeddings import BGEEmbeddings
from .vector_store import ChromaVectorStore
from .mcp_client import TavilyMCPClient


class RAGChain:
    """RAG íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤"""

    # ============================================
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (í´ë˜ìŠ¤ ë ˆë²¨ ìƒìˆ˜)
    # ============================================
    BASE_SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ìƒê¶Œ ë¶„ì„ ë° ì°½ì—… ì»¨ì„¤íŒ… ì „ë¬¸ê°€ì´ì ì¹œê·¼í•œ ëŒ€í™” ìƒëŒ€ì…ë‹ˆë‹¤.

ë‹µë³€ ì „ëµ:
1. **ë¶€ë™ì‚°/ìƒê¶Œ ê´€ë ¨ ì§ˆë¬¸**: {context_instruction}
   - {context_details}
   - êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì¡°ì–¸ ì œê³µ

2. **ì¼ìƒì ì¸ ëŒ€í™”/ì¸ì‚¬/ì¡ë‹´**: ì°¸ê³  ìë£Œì™€ ë¬´ê´€í•˜ê²Œ ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•˜ê²Œ ì‘ë‹µí•˜ì„¸ìš”.
   - "ì•ˆë…•í•˜ì„¸ìš”", "ê³ ë§™ìŠµë‹ˆë‹¤", "ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?" ë“±ì˜ ì§ˆë¬¸ì—ëŠ” ì¼ë°˜ì ì¸ ëŒ€í™”ë¡œ ì‘ë‹µ
   - ì°¸ê³  ìë£Œë¥¼ ì–µì§€ë¡œ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”
   - ì¹œê·¼í•˜ê³  ë”°ëœ»í•œ í†¤ ìœ ì§€

ì¶œë ¥ ìŠ¤íƒ€ì¼ ê°€ì´ë“œ:
- ë§ˆí¬ë‹¤ìš´ íŠ¹ìˆ˜ë¬¸ì(###, ***, ---, ===, ~~~)ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
- ì œëª©ì´ë‚˜ ê°•ì¡°ê°€ í•„ìš”í•  ë•ŒëŠ” **êµµì€ ê¸€ì”¨**ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
- êµ¬ë¶„ì„ (---, ***)ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
- ëª©ë¡ì€ "â€¢" ë˜ëŠ” ìˆ«ìë¡œ ê°„ê²°í•˜ê²Œ í‘œí˜„í•˜ì„¸ìš”
- ë¬¸ë‹¨ êµ¬ë¶„ì€ ë¹ˆ ì¤„ í•˜ë‚˜ë¡œ ì¶©ë¶„í•©ë‹ˆë‹¤
- ìì—°ìŠ¤ëŸ½ê³  ì½ê¸° í¸í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”

ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ë¥¼ íŒŒì•…í•˜ì—¬ ì ì ˆí•œ ë°©ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”."""

    # ì»¨í…ìŠ¤íŠ¸ë³„ instruction
    CONTEXT_INSTRUCTIONS = {
        "local": {
            "instruction": "ì œê³µëœ ì°¸ê³  ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì „ë¬¸ì ì´ê³  êµ¬ì²´ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.",
            "details": "ì°¸ê³  ë¬¸ì„œì˜ ë‚´ìš©ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì„¤ëª…í•˜ê³ , í•„ìš”ì‹œ ì¶œì²˜ë¥¼ ì–¸ê¸‰í•˜ì„¸ìš”. ì°¸ê³  ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì†”ì§í•˜ê²Œ 'ì œê³µëœ ìë£Œì—ëŠ” í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë§í•˜ê¸°"
        },
        "web": {
            "instruction": "ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì‹  ì •ë³´ë¥¼ ë°˜ì˜í•œ ì „ë¬¸ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.",
            "details": "ì›¹ ê²€ìƒ‰ ê²°ê³¼ì˜ ë‚´ìš©ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì„¤ëª…í•˜ê³ , í•„ìš”ì‹œ ì¶œì²˜(URL)ë¥¼ ì–¸ê¸‰í•˜ì„¸ìš”. ìµœì‹  ì •ë³´ë¥¼ ë°˜ì˜í•˜ì—¬ ì‹¤ìš©ì ì¸ ì¡°ì–¸ ì œê³µ"
        },
        "hybrid": {
            "instruction": "ë¡œì»¬ ì§€ì‹ ë°ì´í„°ë² ì´ìŠ¤ì™€ ìµœì‹  ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ëª¨ë‘ í™œìš©í•˜ì—¬ ì „ë¬¸ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.",
            "details": "ë¡œì»¬ ë¬¸ì„œ(ë‚´ë¶€ ìë£Œ)ì™€ ì›¹ ê²€ìƒ‰ ê²°ê³¼(ìµœì‹  ì •ë³´)ë¥¼ ê· í˜•ìˆê²Œ í™œìš©í•˜ê³ , ì •ë³´ì˜ ì¶œì²˜(ë¡œì»¬ ë¬¸ì„œ vs ì›¹)ë¥¼ ëª…í™•íˆ êµ¬ë¶„í•˜ì„¸ìš”. ìµœì‹  íŠ¸ë Œë“œì™€ ê¸°ë³¸ ì§€ì‹ì„ ê²°í•©í•˜ì—¬ ì‹¤ìš©ì ì¸ ì¡°ì–¸ ì œê³µ"
        }
    }

    def __init__(
        self,
        openai_api_key: str = None,
        retriever: Retriever = None,
        model_name: str = "gpt-4o-mini",
        temperature: float = 0.7,
        max_tokens: int = 1000,
        tavily_api_key: str = None,
        enable_web_search: bool = True
    ):
        """
        RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”

        Args:
            openai_api_key: OpenAI API í‚¤
            retriever: ê²€ìƒ‰ê¸° ì¸ìŠ¤í„´ìŠ¤
            model_name: OpenAI ëª¨ë¸ ì´ë¦„
            temperature: ìƒì„± ì˜¨ë„ (0~2)
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            tavily_api_key: Tavily API í‚¤ (ì›¹ ê²€ìƒ‰ìš©)
            enable_web_search: ì›¹ ê²€ìƒ‰ í™œì„±í™” ì—¬ë¶€
        """
        # OpenAI API í‚¤ ì„¤ì •
        if openai_api_key is None:
            openai_api_key = os.getenv("OPENAI_API_KEY")
            if not openai_api_key:
                raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        self.client = OpenAI(api_key=openai_api_key)
        self.model_name = model_name
        self.temperature = temperature
        self.max_tokens = max_tokens

        # ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
        if retriever is None:
            print("ğŸ”§ ê¸°ë³¸ Retriever ì´ˆê¸°í™” ì¤‘...")
            self.retriever = Retriever()
        else:
            self.retriever = retriever

        # Tavily MCP í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.tavily_mcp = None
        self.enable_web_search = enable_web_search

        if enable_web_search:
            if tavily_api_key is None:
                tavily_api_key = os.getenv("TAVILY_API_KEY")

            if tavily_api_key:
                try:
                    self.tavily_mcp = TavilyMCPClient(tavily_api_key)
                    print("ğŸŒ Tavily ì›¹ ê²€ìƒ‰ í™œì„±í™”")
                except Exception as e:
                    print(f"âš ï¸  Tavily ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    print("   â†’ ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥ ë¹„í™œì„±í™”")
                    self.enable_web_search = False
            else:
                print("âš ï¸  TAVILY_API_KEY ì—†ìŒ â†’ ì›¹ ê²€ìƒ‰ ë¹„í™œì„±í™”")
                self.enable_web_search = False

        print(f"[OK] RAG íŒŒì´í”„ë¼ì¸ ì¤€ë¹„ ì™„ë£Œ (ëª¨ë¸: {model_name})")

    def _get_system_prompt(self, mode: str) -> str:
        """
        ì»¨í…ìŠ¤íŠ¸ ëª¨ë“œì— ë§ëŠ” ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±

        Args:
            mode: "local", "web", "hybrid"

        Returns:
            ì™„ì„±ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        """
        if mode not in self.CONTEXT_INSTRUCTIONS:
            raise ValueError(f"Invalid mode: {mode}. Must be one of {list(self.CONTEXT_INSTRUCTIONS.keys())}")

        context_info = self.CONTEXT_INSTRUCTIONS[mode]
        return self.BASE_SYSTEM_PROMPT.format(
            context_instruction=context_info["instruction"],
            context_details=context_info["details"]
        )

    def create_prompt(
        self,
        query: str,
        retrieved_docs: List[Dict[str, Any]],
        conversation_history: Optional[List[Dict[str, str]]] = None
    ) -> List[Dict[str, str]]:
        """
        RAG í”„ë¡¬í”„íŠ¸ ìƒì„± (ë¡œì»¬ ë¬¸ì„œ ì „ìš©)

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            retrieved_docs: ê²€ìƒ‰ëœ ë¬¸ì„œë“¤
            conversation_history: ëŒ€í™” ê¸°ë¡

        Returns:
            OpenAI ë©”ì‹œì§€ í˜•ì‹ì˜ í”„ë¡¬í”„íŠ¸
        """
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (í…œí”Œë¦¿ ì‚¬ìš©)
        system_prompt = self._get_system_prompt("local")

        # ê²€ìƒ‰ëœ ë¬¸ì„œ í¬ë§·íŒ…
        context = self.retriever.format_documents_for_prompt(retrieved_docs)

        # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        user_prompt = f"""[ì°¸ê³  ë¬¸ì„œ]
{context}

[ì‚¬ìš©ì ì§ˆë¬¸]
{query}

ìœ„ ì°¸ê³  ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
"""

        # ë©”ì‹œì§€ êµ¬ì„±
        messages = [{"role": "system", "content": system_prompt}]

        # ëŒ€í™” ê¸°ë¡ ì¶”ê°€ (ìˆìœ¼ë©´)
        if conversation_history:
            messages.extend(conversation_history)

        # í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
        messages.append({"role": "user", "content": user_prompt})

        return messages

    def _build_search_query_with_history(
        self,
        current_query: str,
        conversation_history: Optional[List[Dict[str, str]]] = None,
        max_history: int = 5
    ) -> str:
        """
        ì´ì „ ì‚¬ìš©ì ì§ˆë¬¸ë“¤ì„ í˜„ì¬ ì§ˆë¬¸ì— ì—°ê²°í•˜ì—¬ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
        
        Args:
            current_query: í˜„ì¬ ì§ˆë¬¸
            conversation_history: ëŒ€í™” ê¸°ë¡ (user + assistant)
            max_history: í¬í•¨í•  ìµœëŒ€ ì´ì „ ì§ˆë¬¸ ê°œìˆ˜ (ê¸°ë³¸ 2ê°œ)
        
        Returns:
            í™•ì¥ëœ ê²€ìƒ‰ ì¿¼ë¦¬ (ì´ì „ user ì§ˆë¬¸ë“¤ + í˜„ì¬ ì§ˆë¬¸)
        
        Example:
            conversation_history = [
                {"role": "user", "content": "ê°•ë‚¨ì—­ ë§›ì§‘"},
                {"role": "assistant", "content": "..."},
                {"role": "user", "content": "ê·¸ ê·¼ì²˜ ì•„íŒŒíŠ¸"}
            ]
            current_query = "ê°€ê²©ëŒ€ëŠ”?"
            
            â†’ ê²°ê³¼: "ê°•ë‚¨ì—­ ë§›ì§‘ ê·¸ ê·¼ì²˜ ì•„íŒŒíŠ¸ ê°€ê²©ëŒ€ëŠ”?"
        """
        if not conversation_history:
            return current_query
        
        # ì‚¬ìš©ì ì§ˆë¬¸ë§Œ ì¶”ì¶œ (role="user")
        # assistant ë‹µë³€ì€ ì œì™¸ (ê²€ìƒ‰ íš¨ìœ¨ì„± ë° ì„ë² ë”© í’ˆì§ˆ í–¥ìƒ)
        user_queries = [
            msg["content"] 
            for msg in conversation_history 
            if msg.get("role") == "user"
        ][-max_history:]  # ìµœê·¼ Nê°œë§Œ ì„ íƒ
        
        if not user_queries:
            return current_query
        
        # ì´ì „ ì§ˆë¬¸ë“¤ + í˜„ì¬ ì§ˆë¬¸ ì—°ê²°
        combined_query = " ".join(user_queries + [current_query])
        
        print(f"[SEARCH QUERY] ì›ë³¸: {current_query}")
        print(f"[SEARCH QUERY] í™•ì¥: {combined_query}")
        
        return combined_query

    def _is_realtime_query(self, query: str) -> bool:
        """
        ì‹¤ì‹œê°„ ì •ë³´ê°€ í•„ìš”í•œ ì§ˆë¬¸ì¸ì§€ íŒë‹¨

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸

        Returns:
            ì‹¤ì‹œê°„ ì •ë³´ í•„ìš” ì—¬ë¶€
        """
        realtime_keywords = [
            "ìµœì‹ ", "í˜„ì¬", "ì§€ê¸ˆ", "ìš”ì¦˜", "íŠ¸ë Œë“œ",
            "2025", "2024", "ì˜¬í•´", "ì´ë²ˆ ë‹¬", "ìµœê·¼",
            "ì˜¤ëŠ˜", "ì–´ì œ", "ë‚´ì¼"
        ]
        query_lower = query.lower()
        return any(keyword in query_lower for keyword in realtime_keywords)

    async def _tavily_search(
        self,
        query: str,
        search_depth: str = "advanced",
        max_results: int = 5,
        topic: str = "general",
        days: Optional[int] = None
    ) -> Optional[Dict[str, Any]]:
        """
        Tavily ì›¹ ê²€ìƒ‰ ì‹¤í–‰

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            search_depth: ê²€ìƒ‰ ê¹Šì´
            max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜
            topic: ê²€ìƒ‰ ì£¼ì œ
            days: ê²€ìƒ‰ ê¸°ê°„ (ì¼)

        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë˜ëŠ” None
        """
        if not self.enable_web_search or not self.tavily_mcp:
            return None

        try:
            result = await self.tavily_mcp.search(
                query=query,
                search_depth=search_depth,
                max_results=max_results,
                topic=topic,
                days=days
            )
            return result
        except Exception as e:
            print(f"[ERROR] Tavily ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return None

    def _generate_from_docs(
        self,
        local_docs: List[Dict[str, Any]],
        query: str,
        conversation_history: Optional[List[Dict[str, str]]] = None
    ) -> Dict[str, Any]:
        """
        ë¡œì»¬ ë¬¸ì„œë§Œ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„± (ê¸°ì¡´ RAG ë¡œì§)

        Args:
            local_docs: ê²€ìƒ‰ëœ ë¡œì»¬ ë¬¸ì„œ
            query: ì‚¬ìš©ì ì§ˆë¬¸
            conversation_history: ëŒ€í™” ê¸°ë¡

        Returns:
            ë‹µë³€ ê²°ê³¼
        """
        print("[GENERATE] ì „ëµ: ë¡œì»¬ ë¬¸ì„œë§Œ ì‚¬ìš© (RAG)")

        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        messages = self.create_prompt(query, local_docs, conversation_history)

        # LLM í˜¸ì¶œ
        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens
            )

            answer = response.choices[0].message.content

            return {
                "answer": answer,
                "sources": local_docs,
                "web_search_used": False,
                "query": query,
                "usage": {
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens
                }
            }

        except Exception as e:
            print(f"[ERROR] LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return {
                "answer": f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "sources": local_docs,
                "web_search_used": False,
                "query": query
            }

    def _generate_from_web(
        self,
        web_results: Dict[str, Any],
        query: str,
        conversation_history: Optional[List[Dict[str, str]]] = None
    ) -> Dict[str, Any]:
        """
        ì›¹ ê²€ìƒ‰ ê²°ê³¼ë§Œ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„±

        Args:
            web_results: Tavily ê²€ìƒ‰ ê²°ê³¼
            query: ì‚¬ìš©ì ì§ˆë¬¸
            conversation_history: ëŒ€í™” ê¸°ë¡

        Returns:
            ë‹µë³€ ê²°ê³¼
        """
        print("[GENERATE] ì „ëµ: ì›¹ ê²€ìƒ‰ ê²°ê³¼ë§Œ ì‚¬ìš© (Tavily)")

        # ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        web_context = self.tavily_mcp.format_search_results_for_prompt(web_results, max_results=3)

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (í…œí”Œë¦¿ ì‚¬ìš©)
        system_prompt = self._get_system_prompt("web")

        # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
        user_prompt = f"""[ì›¹ ê²€ìƒ‰ ê²°ê³¼]
{web_context}

[ì‚¬ìš©ì ì§ˆë¬¸]
{query}

ìœ„ ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
"""

        # ë©”ì‹œì§€ êµ¬ì„±
        messages = [{"role": "system", "content": system_prompt}]

        if conversation_history:
            messages.extend(conversation_history)

        messages.append({"role": "user", "content": user_prompt})

        # LLM í˜¸ì¶œ
        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens
            )

            answer = response.choices[0].message.content

            return {
                "answer": answer,
                "sources": [],
                "web_results": web_results,
                "web_search_used": True,
                "query": query,
                "usage": {
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens
                }
            }

        except Exception as e:
            print(f"[ERROR] LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return {
                "answer": f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "sources": [],
                "web_results": web_results,
                "web_search_used": True,
                "query": query
            }

    def _generate_hybrid(
        self,
        local_docs: List[Dict[str, Any]],
        web_results: Dict[str, Any],
        query: str,
        conversation_history: Optional[List[Dict[str, str]]] = None
    ) -> Dict[str, Any]:
        """
        ë¡œì»¬ ë¬¸ì„œ + ì›¹ ê²€ìƒ‰ ê²°ê³¼ ê²°í•©í•˜ì—¬ ë‹µë³€ ìƒì„±

        Args:
            local_docs: ë¡œì»¬ ê²€ìƒ‰ ê²°ê³¼
            web_results: ì›¹ ê²€ìƒ‰ ê²°ê³¼
            query: ì‚¬ìš©ì ì§ˆë¬¸
            conversation_history: ëŒ€í™” ê¸°ë¡

        Returns:
            ë‹µë³€ ê²°ê³¼
        """
        print("[GENERATE] ì „ëµ: í•˜ì´ë¸Œë¦¬ë“œ (ë¡œì»¬ + ì›¹)")

        # ë¡œì»¬ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸
        local_context = self.retriever.format_documents_for_prompt(local_docs)

        # ì›¹ ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸
        web_context = self.tavily_mcp.format_search_results_for_prompt(web_results, max_results=2)

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (í…œí”Œë¦¿ ì‚¬ìš©)
        system_prompt = self._get_system_prompt("hybrid")

        # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
        user_prompt = f"""[ë‚´ë¶€ ì°¸ê³  ë¬¸ì„œ]
{local_context}

[ìµœì‹  ì›¹ ê²€ìƒ‰ ê²°ê³¼]
{web_context}

[ì‚¬ìš©ì ì§ˆë¬¸]
{query}

ìœ„ì˜ ë‚´ë¶€ ì°¸ê³  ë¬¸ì„œì™€ ìµœì‹  ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
"""

        # ë©”ì‹œì§€ êµ¬ì„±
        messages = [{"role": "system", "content": system_prompt}]

        if conversation_history:
            messages.extend(conversation_history)

        messages.append({"role": "user", "content": user_prompt})

        # LLM í˜¸ì¶œ
        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens
            )

            answer = response.choices[0].message.content

            return {
                "answer": answer,
                "sources": local_docs,
                "web_results": web_results,
                "web_search_used": True,
                "query": query,
                "usage": {
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens
                }
            }

        except Exception as e:
            print(f"[ERROR] LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return {
                "answer": f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "sources": local_docs,
                "web_results": web_results,
                "web_search_used": True,
                "query": query
            }

    async def run(
        self,
        query: str,
        conversation_history: Optional[List[Dict[str, str]]] = None,
        top_k: int = 3
    ) -> Dict[str, Any]:
        """
        RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (Tavily ì›¹ ê²€ìƒ‰ í†µí•©)

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            conversation_history: ëŒ€í™” ê¸°ë¡
            top_k: ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜

        Returns:
            {
                "answer": "LLM ë‹µë³€",
                "sources": [{...}, {...}],  # ì°¸ê³  ë¬¸ì„œ
                "web_results": {...},       # ì›¹ ê²€ìƒ‰ ê²°ê³¼ (ìˆìœ¼ë©´)
                "web_search_used": bool,    # ì›¹ ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€
                "query": "ì›ë³¸ ì§ˆë¬¸"
            }
        """
        print(f"\n[SEARCH] RAG íŒŒì´í”„ë¼ì¸ ì‹œì‘: {query}")

        # 0. ê²€ìƒ‰ìš© ì¿¼ë¦¬ ìƒì„± (ì´ì „ ì§ˆë¬¸ í¬í•¨)
        search_query = self._build_search_query_with_history(
            query, 
            conversation_history,
            max_history=2  # ìµœê·¼ 2ê°œ ì§ˆë¬¸ë§Œ í¬í•¨
        )

        # 1. ë¡œì»¬ ë¬¸ì„œ ê²€ìƒ‰ (í™•ì¥ëœ ì¿¼ë¦¬ ì‚¬ìš©)
        print(f"[DOCS] 1ë‹¨ê³„: ë¡œì»¬ ë¬¸ì„œ ê²€ìƒ‰ (Top-{top_k})...")
        local_docs = self.retriever.search(search_query, top_k=top_k)
        print(f"   âœ“ {len(local_docs)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ")

        # 2. ì‹¤ì‹œê°„ ì •ë³´ í•„ìš” ì—¬ë¶€ íŒë‹¨
        needs_realtime = self._is_realtime_query(query)
        print(f"[CHECK] ì‹¤ì‹œê°„ ì •ë³´ í•„ìš”: {'âœ… ì˜ˆ' if needs_realtime else 'âŒ ì•„ë‹ˆì˜¤'}")

        # 3. ì „ëµ ì„ íƒ ë° ì‹¤í–‰
        if local_docs and not needs_realtime:
            # Case A: ë¡œì»¬ ë¬¸ì„œ ì¶©ë¶„ + ì‹¤ì‹œê°„ ë¶ˆí•„ìš” â†’ ë¡œì»¬ë§Œ ì‚¬ìš©
            print(f"[STRATEGY] Case A: ë¡œì»¬ ë¬¸ì„œë§Œ ì‚¬ìš©")
            return self._generate_from_docs(local_docs, query, conversation_history)

        elif needs_realtime:
            # Case D: ì‹¤ì‹œê°„ í•„ìš” â†’ í•˜ì´ë¸Œë¦¬ë“œ ë˜ëŠ” ì›¹ë§Œ
            print(f"[STRATEGY] Case D: ì‹¤ì‹œê°„ ì •ë³´ í•„ìš” â†’ ì›¹ ê²€ìƒ‰ ì‹¤í–‰")
            web_results = await self._tavily_search(search_query, search_depth="advanced", max_results=5)

            if web_results:
                if local_docs:
                    # ë¡œì»¬ + ì›¹ í•˜ì´ë¸Œë¦¬ë“œ
                    return self._generate_hybrid(local_docs, web_results, query, conversation_history)
                else:
                    # ì›¹ë§Œ
                    return self._generate_from_web(web_results, query, conversation_history)
            else:
                # ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨ â†’ ë¡œì»¬ë§Œ ì‚¬ìš© (ìˆìœ¼ë©´)
                if local_docs:
                    return self._generate_from_docs(local_docs, query, conversation_history)
                else:
                    return {
                        "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                        "sources": [],
                        "web_search_used": False,
                        "query": query
                    }

        else:
            # Case C: ë¡œì»¬ ì—†ìŒ + ì‹¤ì‹œê°„ ë¶ˆí•„ìš” â†’ Tavily í´ë°±
            print(f"[STRATEGY] Case C: ë¡œì»¬ ë¬¸ì„œ ì—†ìŒ â†’ Tavily í´ë°±")
            web_results = await self._tavily_search(search_query, search_depth="advanced", max_results=5)

            if web_results:
                return self._generate_from_web(web_results, query, conversation_history)
            else:
                return {
                    "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "sources": [],
                    "web_search_used": False,
                    "query": query
                }

    async def stream_run(
        self,
        query: str,
        conversation_history: Optional[List[Dict[str, str]]] = None,
        top_k: int = 3
    ):
        """
        RAG íŒŒì´í”„ë¼ì¸ ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ (Tavily ì›¹ ê²€ìƒ‰ í†µí•©)

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            conversation_history: ëŒ€í™” ê¸°ë¡
            top_k: ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜

        Yields:
            ë‹µë³€ ì²­í¬ ë˜ëŠ” ë©”íƒ€ë°ì´í„°
        """
        print(f"\n[SEARCH] RAG íŒŒì´í”„ë¼ì¸ ì‹œì‘ (ìŠ¤íŠ¸ë¦¬ë°): {query}")

        # 0. ê²€ìƒ‰ìš© ì¿¼ë¦¬ ìƒì„± (ì´ì „ ì§ˆë¬¸ í¬í•¨)
        search_query = self._build_search_query_with_history(
            query, 
            conversation_history,
            max_history=2  # ìµœê·¼ 2ê°œ ì§ˆë¬¸ë§Œ í¬í•¨
        )

        # 1. ë¡œì»¬ ë¬¸ì„œ ê²€ìƒ‰ (í™•ì¥ëœ ì¿¼ë¦¬ ì‚¬ìš©)
        print(f"[DOCS] 1ë‹¨ê³„: ë¡œì»¬ ë¬¸ì„œ ê²€ìƒ‰ (Top-{top_k})...")
        local_docs = self.retriever.search(search_query, top_k=top_k)
        print(f"   âœ“ {len(local_docs)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ")

        # 2. ì‹¤ì‹œê°„ ì •ë³´ í•„ìš” ì—¬ë¶€ íŒë‹¨
        needs_realtime = self._is_realtime_query(query)
        print(f"[CHECK] ì‹¤ì‹œê°„ ì •ë³´ í•„ìš”: {'âœ… ì˜ˆ' if needs_realtime else 'âŒ ì•„ë‹ˆì˜¤'}")

        # 3. ì „ëµ ì„ íƒ ë° ì‹¤í–‰
        if local_docs and not needs_realtime:
            # Case A: ë¡œì»¬ ë¬¸ì„œë§Œ ì‚¬ìš©
            print(f"[STRATEGY] Case A: ë¡œì»¬ ë¬¸ì„œë§Œ ì‚¬ìš© (ìŠ¤íŠ¸ë¦¬ë°)")
            yield {"type": "sources", "content": local_docs}
            async for chunk in self._stream_from_docs(local_docs, query, conversation_history):
                yield chunk

        elif needs_realtime:
            # Case D: ì‹¤ì‹œê°„ í•„ìš” â†’ ì›¹ ê²€ìƒ‰
            print(f"[STRATEGY] Case D: ì‹¤ì‹œê°„ ì •ë³´ í•„ìš” â†’ ì›¹ ê²€ìƒ‰ ì‹¤í–‰")
            web_results = await self._tavily_search(search_query, search_depth="advanced", max_results=5)

            if web_results:
                if local_docs:
                    # í•˜ì´ë¸Œë¦¬ë“œ
                    print(f"[STREAM] í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘")
                    yield {"type": "sources", "content": local_docs}
                    yield {"type": "web_results", "content": web_results}
                    async for chunk in self._stream_hybrid(local_docs, web_results, query, conversation_history):
                        yield chunk
                else:
                    # ì›¹ë§Œ
                    print(f"[STREAM] ì›¹ ê²€ìƒ‰ ê²°ê³¼ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘")
                    yield {"type": "web_results", "content": web_results}
                    async for chunk in self._stream_from_web(web_results, query, conversation_history):
                        yield chunk
            else:
                # ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨ â†’ ë¡œì»¬ë§Œ (ìˆìœ¼ë©´)
                if local_docs:
                    yield {"type": "sources", "content": local_docs}
                    async for chunk in self._stream_from_docs(local_docs, query, conversation_history):
                        yield chunk
                else:
                    yield {
                        "type": "answer",
                        "content": "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                    }

        else:
            # Case C: ë¡œì»¬ ì—†ìŒ â†’ Tavily í´ë°±
            print(f"[STRATEGY] Case C: ë¡œì»¬ ë¬¸ì„œ ì—†ìŒ â†’ Tavily í´ë°±")
            web_results = await self._tavily_search(search_query, search_depth="advanced", max_results=5)

            if web_results:
                yield {"type": "web_results", "content": web_results}
                async for chunk in self._stream_from_web(web_results, query, conversation_history):
                    yield chunk
            else:
                yield {
                    "type": "answer",
                    "content": "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                }

    async def _stream_from_docs(
        self,
        local_docs: List[Dict[str, Any]],
        query: str,
        conversation_history: Optional[List[Dict[str, str]]] = None
    ):
        """
        ë¡œì»¬ ë¬¸ì„œë§Œ ì‚¬ìš©í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„±

        Args:
            local_docs: ê²€ìƒ‰ëœ ë¡œì»¬ ë¬¸ì„œ
            query: ì‚¬ìš©ì ì§ˆë¬¸
            conversation_history: ëŒ€í™” ê¸°ë¡

        Yields:
            ë‹µë³€ ì²­í¬
        """
        print("[STREAM] ë¡œì»¬ ë¬¸ì„œ ê¸°ë°˜ ìŠ¤íŠ¸ë¦¬ë°")
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        messages = self.create_prompt(query, local_docs, conversation_history)

        # LLM ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
        try:
            stream = self.client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                stream=True
            )

            for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield {
                        "type": "answer",
                        "content": chunk.choices[0].delta.content
                    }

        except Exception as e:
            print(f"[ERROR] LLM ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨: {e}")
            yield {
                "type": "error",
                "content": str(e)
            }

    async def _stream_from_web(
        self,
        web_results: Dict[str, Any],
        query: str,
        conversation_history: Optional[List[Dict[str, str]]] = None
    ):
        """
        ì›¹ ê²€ìƒ‰ ê²°ê³¼ë§Œ ì‚¬ìš©í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„±

        Args:
            web_results: Tavily ê²€ìƒ‰ ê²°ê³¼
            query: ì‚¬ìš©ì ì§ˆë¬¸
            conversation_history: ëŒ€í™” ê¸°ë¡

        Yields:
            ë‹µë³€ ì²­í¬
        """
        print("[STREAM] ì›¹ ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜ ìŠ¤íŠ¸ë¦¬ë°")

        # ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        web_context = self.tavily_mcp.format_search_results_for_prompt(web_results, max_results=3)

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (í…œí”Œë¦¿ ì‚¬ìš©)
        system_prompt = self._get_system_prompt("web")

        # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
        user_prompt = f"""[ì›¹ ê²€ìƒ‰ ê²°ê³¼]
{web_context}

[ì‚¬ìš©ì ì§ˆë¬¸]
{query}

ìœ„ ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
"""

        # ë©”ì‹œì§€ êµ¬ì„±
        messages = [{"role": "system", "content": system_prompt}]

        if conversation_history:
            messages.extend(conversation_history)

        messages.append({"role": "user", "content": user_prompt})

        # LLM ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
        try:
            stream = self.client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                stream=True
            )

            for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield {
                        "type": "answer",
                        "content": chunk.choices[0].delta.content
                    }

        except Exception as e:
            print(f"[ERROR] LLM ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨: {e}")
            yield {
                "type": "error",
                "content": str(e)
            }

    async def _stream_hybrid(
        self,
        local_docs: List[Dict[str, Any]],
        web_results: Dict[str, Any],
        query: str,
        conversation_history: Optional[List[Dict[str, str]]] = None
    ):
        """
        ë¡œì»¬ ë¬¸ì„œ + ì›¹ ê²€ìƒ‰ ê²°ê³¼ ê²°í•©í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„±

        Args:
            local_docs: ë¡œì»¬ ê²€ìƒ‰ ê²°ê³¼
            web_results: ì›¹ ê²€ìƒ‰ ê²°ê³¼
            query: ì‚¬ìš©ì ì§ˆë¬¸
            conversation_history: ëŒ€í™” ê¸°ë¡

        Yields:
            ë‹µë³€ ì²­í¬
        """
        print("[STREAM] í•˜ì´ë¸Œë¦¬ë“œ (ë¡œì»¬ + ì›¹) ìŠ¤íŠ¸ë¦¬ë°")

        # ë¡œì»¬ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸
        local_context = self.retriever.format_documents_for_prompt(local_docs)

        # ì›¹ ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸
        web_context = self.tavily_mcp.format_search_results_for_prompt(web_results, max_results=2)

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (í…œí”Œë¦¿ ì‚¬ìš©)
        system_prompt = self._get_system_prompt("hybrid")

        # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
        user_prompt = f"""[ë‚´ë¶€ ì°¸ê³  ë¬¸ì„œ]
{local_context}

[ìµœì‹  ì›¹ ê²€ìƒ‰ ê²°ê³¼]
{web_context}

[ì‚¬ìš©ì ì§ˆë¬¸]
{query}

ìœ„ì˜ ë‚´ë¶€ ì°¸ê³  ë¬¸ì„œì™€ ìµœì‹  ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
"""

        # ë©”ì‹œì§€ êµ¬ì„±
        messages = [{"role": "system", "content": system_prompt}]

        if conversation_history:
            messages.extend(conversation_history)

        messages.append({"role": "user", "content": user_prompt})

        # LLM ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
        try:
            stream = self.client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                stream=True
            )

            for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield {
                        "type": "answer",
                        "content": chunk.choices[0].delta.content
                    }

        except Exception as e:
            print(f"[ERROR] LLM ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨: {e}")
            yield {
                "type": "error",
                "content": str(e)
            }


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # RAG ì²´ì¸ ì´ˆê¸°í™”
    rag_chain = RAGChain()

    # ì§ˆë¬¸
    query = "ê°•ë‚¨ì—ì„œ ì¹´í˜ë¥¼ ì°½ì—…í•˜ë ¤ê³  í•˜ëŠ”ë° ì–´ë–¤ ì ì„ ê³ ë ¤í•´ì•¼ í•˜ë‚˜ìš”?"

    # ì‹¤í–‰
    result = rag_chain.run(query, top_k=3)

    # ê²°ê³¼ ì¶œë ¥
    print(f"\n{'='*60}")
    print(f"ì§ˆë¬¸: {result['query']}")
    print(f"\në‹µë³€:\n{result['answer']}")
    print(f"\nì°¸ê³  ë¬¸ì„œ ({len(result['sources'])}ê°œ):")
    for i, source in enumerate(result['sources']):
        print(f"  [{i+1}] {source['metadata'].get('source', 'unknown')} (ìœ ì‚¬ë„: {source['score']:.3f})")
