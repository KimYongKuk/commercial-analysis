"""
BGE-M3-KO ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ë²¡í„°í™” ëª¨ë“ˆ

BGE-M3-KOëŠ” í•œêµ­ì–´ì— ìµœì í™”ëœ ì„ë² ë”© ëª¨ë¸ì…ë‹ˆë‹¤.
HuggingFace: dragonkue/BGE-m3-ko
"""

from sentence_transformers import SentenceTransformer
from typing import List, Union
import torch


class BGEEmbeddings:
    """BGE-M3-KO ì„ë² ë”© ëª¨ë¸ ë˜í¼ í´ë˜ìŠ¤"""

    def __init__(
        self,
        model_name: str = "dragonkue/BGE-m3-ko",
        device: str = None
    ):
        """
        BGE-M3-KO ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”

        Args:
            model_name: HuggingFace ëª¨ë¸ ì´ë¦„ (ê¸°ë³¸ê°’: dragonkue/BGE-m3-ko)
            device: ì‹¤í–‰ ë””ë°”ì´ìŠ¤ ('cuda', 'cpu', None=ìë™ê°ì§€)
        """
        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ ìë™ ê°ì§€
        if device is None:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device

        print(f"ğŸš€ BGE-M3-KO ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘... (device: {self.device})")

        try:
            # SentenceTransformer ëª¨ë¸ ë¡œë“œ
            self.model = SentenceTransformer(model_name, device=self.device)
            print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}")
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise

    def embed_query(self, text: str) -> List[float]:
        """
        ë‹¨ì¼ ì¿¼ë¦¬ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜

        Args:
            text: ì„ë² ë”©í•  í…ìŠ¤íŠ¸

        Returns:
            ì„ë² ë”© ë²¡í„° (list of floats)
        """
        if not text or not text.strip():
            raise ValueError("í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")

        # ì„ë² ë”© ìƒì„±
        embedding = self.model.encode(
            text,
            convert_to_numpy=True,
            normalize_embeddings=True  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°ì„ ìœ„í•œ ì •ê·œí™”
        )

        return embedding.tolist()

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """
        ì—¬ëŸ¬ ë¬¸ì„œë¥¼ ë°°ì¹˜ë¡œ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜

        Args:
            texts: ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
        """
        if not texts:
            return []

        # ë¹ˆ í…ìŠ¤íŠ¸ í•„í„°ë§
        valid_texts = [t for t in texts if t and t.strip()]
        if not valid_texts:
            raise ValueError("ìœ íš¨í•œ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

        # ë°°ì¹˜ ì„ë² ë”© ìƒì„±
        embeddings = self.model.encode(
            valid_texts,
            convert_to_numpy=True,
            normalize_embeddings=True,
            show_progress_bar=len(valid_texts) > 10,  # 10ê°œ ì´ìƒì¼ ë•Œë§Œ ì§„í–‰ë°” í‘œì‹œ
            batch_size=32  # ë°°ì¹˜ í¬ê¸°
        )

        return embeddings.tolist()

    def get_embedding_dimension(self) -> int:
        """ì„ë² ë”© ë²¡í„°ì˜ ì°¨ì› ìˆ˜ ë°˜í™˜"""
        return self.model.get_sentence_embedding_dimension()


# LangChain í˜¸í™˜ ì„ë² ë”© í´ë˜ìŠ¤
class LangChainBGEEmbeddings:
    """
    LangChainê³¼ í˜¸í™˜ë˜ëŠ” BGE-M3-KO ì„ë² ë”© í´ë˜ìŠ¤

    LangChainì˜ Embeddings ì¸í„°í˜ì´ìŠ¤ë¥¼ êµ¬í˜„í•˜ì—¬
    LangChainì˜ ë‹¤ë¥¸ ì»´í¬ë„ŒíŠ¸ì™€ í•¨ê»˜ ì‚¬ìš© ê°€ëŠ¥
    """

    def __init__(
        self,
        model_name: str = "dragonkue/BGE-m3-ko",
        device: str = None
    ):
        """
        Args:
            model_name: HuggingFace ëª¨ë¸ ì´ë¦„
            device: ì‹¤í–‰ ë””ë°”ì´ìŠ¤
        """
        self.embeddings = BGEEmbeddings(model_name=model_name, device=device)

    def embed_query(self, text: str) -> List[float]:
        """LangChain í˜¸í™˜ ì¿¼ë¦¬ ì„ë² ë”©"""
        return self.embeddings.embed_query(text)

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """LangChain í˜¸í™˜ ë¬¸ì„œ ì„ë² ë”©"""
        return self.embeddings.embed_documents(texts)


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ê¸°ë³¸ ì‚¬ìš©
    embeddings = BGEEmbeddings()

    # ë‹¨ì¼ ì¿¼ë¦¬ ì„ë² ë”©
    query = "ì„œìš¸ ê°•ë‚¨êµ¬ ìƒê¶Œ ë¶„ì„"
    query_embedding = embeddings.embed_query(query)
    print(f"ì¿¼ë¦¬ ì„ë² ë”© ì°¨ì›: {len(query_embedding)}")
    print(f"ì„ë² ë”© ë²¡í„° (ì¼ë¶€): {query_embedding[:5]}")

    # ë¬¸ì„œ ë°°ì¹˜ ì„ë² ë”©
    documents = [
        "ê°•ë‚¨ì—­ ì£¼ë³€ì€ ìœ ë™ì¸êµ¬ê°€ ë§ìŠµë‹ˆë‹¤.",
        "ì„ëŒ€ë£Œê°€ ë¹„êµì  ë†’ì€ í¸ì…ë‹ˆë‹¤.",
        "ê²½ìŸì—…ì²´ê°€ ë§ì•„ ì‹œì¥ ì§„ì…ì´ ì–´ë µìŠµë‹ˆë‹¤."
    ]
    doc_embeddings = embeddings.embed_documents(documents)
    print(f"\në¬¸ì„œ ê°œìˆ˜: {len(doc_embeddings)}")
    print(f"ê° ë¬¸ì„œ ì„ë² ë”© ì°¨ì›: {len(doc_embeddings[0])}")
