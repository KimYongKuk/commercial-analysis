# FastAPI ì„œë²„ ë©”ì¸ íŒŒì¼
# OpenAI APIì™€ ì—°ê²°ëœ ì‹¤ì œ ì±—ë´‡ ì„œë²„!

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from openai import OpenAI
from dotenv import load_dotenv
from typing import Optional, List, Dict, Any
import os
import httpx
import json

# RAG ëª¨ë“ˆ import
from rag.rag_chain import RAGChain

# ============================================
# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# ============================================
# .env íŒŒì¼ì—ì„œ API í‚¤ ì½ì–´ì˜¤ê¸°
load_dotenv()

# OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
# os.getenv("OPENAI_API_KEY") = .env íŒŒì¼ì˜ OPENAI_API_KEY ê°’ì„ ì½ìŒ
client = OpenAI(
    api_key=os.getenv("OPENAI_API_KEY")
)

# ============================================
# RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” (Lazy Loading)
# ============================================
# ì²« ìš”ì²­ ì‹œì—ë§Œ ì´ˆê¸°í™”ë˜ì–´ ì‹œì‘ ì†ë„ ê°œì„ 
import asyncio

# ============================================
# RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” (Lazy Loading)
# ============================================
# ì²« ìš”ì²­ ì‹œì—ë§Œ ì´ˆê¸°í™”ë˜ì–´ ì‹œì‘ ì†ë„ ê°œì„ 
rag_chain = None

async def get_rag_chain():
    """RAG ì²´ì¸ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (Lazy Loading)"""
    global rag_chain
    if rag_chain is None:
        print("ğŸš€ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘... (ì²« ìš”ì²­, 10~20ì´ˆ ì†Œìš”)")
        rag_chain = RAGChain(
            openai_api_key=os.getenv("OPENAI_API_KEY"),
            model_name="gpt-4o-mini",
            temperature=0.7,
            max_tokens=1000
        )

        # MCP ë„êµ¬ëŠ” ì²« ìš”ì²­ ì‹œ ìë™ ë°œê²¬ë©ë‹ˆë‹¤ (Lazy Loading in RAGChain)
        print("[OK] RAG system ready! (MCP ë„êµ¬ëŠ” ì²« ìš”ì²­ ì‹œ ë°œê²¬ë©ë‹ˆë‹¤)")
    return rag_chain

# ============================================
# FastAPI ì•± ìƒì„±
# ============================================
app = FastAPI(
    title="JobFlex Chatbot API",
    description="ìƒê¶Œ ë¶„ì„ ì±—ë´‡ ë°±ì—”ë“œ (OpenAI ì—°ê²°)",
    version="2.0.0"  # OpenAI ì—°ê²° ì™„ë£Œ!
)

# ============================================
# CORS ì„¤ì •
# ============================================
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:5173",  # React ê°œë°œ ì„œë²„
        "http://localhost:5174",
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================
# Pydantic ëª¨ë¸ (ë°ì´í„° ê²€ì¦)
# ============================================
class ChatRequest(BaseModel):
    """
    í´ë¼ì´ì–¸íŠ¸ì—ì„œ ë³´ë‚´ëŠ” ìš”ì²­ í˜•ì‹
    """
    message: str  # ì‚¬ìš©ì ë©”ì‹œì§€
    analysis_results: Optional[List[Dict[str, Any]]] = None  # ë¶„ì„ ê²°ê³¼ (ì„ íƒì )
    conversation_history: Optional[List[Dict[str, str]]] = None  # ëŒ€í™” íˆìŠ¤í† ë¦¬ (ì„ íƒì )

class ChatResponse(BaseModel):
    """
    ì„œë²„ì—ì„œ ë°˜í™˜í•˜ëŠ” ì‘ë‹µ í˜•ì‹
    """
    reply: str    # AI ì±—ë´‡ì˜ ë‹µë³€
    message: str  # ì›ë³¸ ë©”ì‹œì§€ (ë””ë²„ê¹…ìš©)

class RAGChatResponse(BaseModel):
    """
    RAG ì±—ë´‡ ì‘ë‹µ í˜•ì‹ (ì°¸ê³  ë¬¸ì„œ í¬í•¨)
    """
    reply: str                           # AI ì±—ë´‡ì˜ ë‹µë³€
    message: str                         # ì›ë³¸ ë©”ì‹œì§€
    sources: Optional[List[Dict[str, Any]]] = []  # ì°¸ê³  ë¬¸ì„œë“¤
    usage: Optional[Dict[str, int]] = None        # í† í° ì‚¬ìš©ëŸ‰

# ============================================
# ì—”ë“œí¬ì¸íŠ¸: ì„œë²„ ìƒíƒœ ì²´í¬
# ============================================
@app.get("/")
async def root():
    return {
        "message": "JobFlex Chatbot API (OpenAI ì—°ê²° ì™„ë£Œ!)",
        "status": "healthy",
        "version": "2.0.0",
        "ai_model": "gpt-3.5-turbo"
    }

@app.get("/health")
async def health_check():
    return {"status": "ok"}

# ============================================
# ë©”ì¸ ì±—ë´‡ ì—”ë“œí¬ì¸íŠ¸ (OpenAI ì—°ê²°!)
# ============================================
@app.post("/api/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    ì‹¤ì œ OpenAI APIë¥¼ ì‚¬ìš©í•œ ì±—ë´‡ ì—”ë“œí¬ì¸íŠ¸

    ì‘ë™ ë°©ì‹:
    1. ì‚¬ìš©ì ë©”ì‹œì§€ ë°›ê¸°
    2. OpenAI APIë¡œ ë©”ì‹œì§€ ë³´ë‚´ê¸°
    3. AI ì‘ë‹µ ë°›ì•„ì„œ í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ì „ë‹¬
    """
    try:
        user_message = request.message
        analysis_results = request.analysis_results
        conversation_history = request.conversation_history

        # Logging
        print("\n" + "="*50)
        print("[Backend] Received user message:", user_message)
        print("[Backend] History count:", len(conversation_history) if conversation_history else 0)
        if conversation_history:
            print("[Backend] History:")
            for i, msg in enumerate(conversation_history, 1):
                role = msg.get('role', 'unknown')
                content = msg.get('content', '')[:50]
                print(f"  {i}. [{role}] {content}...")
        print("="*50 + "\n")

        # ============================================
        # System Prompt ìƒì„± (ë¶„ì„ ê²°ê³¼ í¬í•¨)
        # ============================================
        base_prompt = """ë‹¹ì‹ ì€ JobFlexì˜ ìƒê¶Œ ë¶„ì„ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì—­í• :
- ì°½ì—…ì„ ê³ ë ¤í•˜ëŠ” ì‚¬ìš©ìì—ê²Œ ìƒê¶Œ ë¶„ì„ ë„ì›€
- ì—…ì¢…, ìœ„ì¹˜, ì˜ˆì‚° ë“±ì— ëŒ€í•œ ì¡°ì–¸ ì œê³µ
- ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ í†¤ìœ¼ë¡œ ëŒ€í™”
- í•œêµ­ì–´ë¡œ ì‘ë‹µ

ë‹µë³€ ì „ëµ:
1. **ë¶€ë™ì‚°/ìƒê¶Œ ê´€ë ¨ ì§ˆë¬¸**: ì „ë¬¸ì ì´ê³  êµ¬ì²´ì ì¸ ì¡°ì–¸ì„ ì œê³µí•˜ì„¸ìš”.
   - ë¶„ì„ ê²°ê³¼ê°€ ìˆìœ¼ë©´ í™œìš©
   - ì‹¤ìš©ì ì´ê³  êµ¬ì²´ì ì¸ ë‹µë³€ ì œê³µ

2. **ì¼ìƒì ì¸ ëŒ€í™”/ì¸ì‚¬/ì¡ë‹´**: ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•˜ê²Œ ì‘ë‹µí•˜ì„¸ìš”.
   - "ì•ˆë…•í•˜ì„¸ìš”", "ê³ ë§™ìŠµë‹ˆë‹¤", "ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?" ë“±ì˜ ì§ˆë¬¸ì—ëŠ” ì¼ë°˜ì ì¸ ëŒ€í™”ë¡œ ì‘ë‹µ
   - ìƒê¶Œ ë¶„ì„ê³¼ ë¬´ê´€í•œ ëŒ€í™”ëŠ” ì–µì§€ë¡œ ì—°ê²°í•˜ì§€ ë§ˆì„¸ìš”
   - ì¹œê·¼í•˜ê³  ë”°ëœ»í•œ í†¤ ìœ ì§€

ì‘ë‹µ ìŠ¤íƒ€ì¼:
- ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ (2-3ë¬¸ì¥)
- ì‹¤ìš©ì ì¸ ì¡°ì–¸ ì¤‘ì‹¬
- ì´ëª¨ì§€ ì ì ˆíˆ ì‚¬ìš© (ğŸ˜Š, ğŸ“Š, ğŸ’¡ ë“±)"""

        # ë¶„ì„ ê²°ê³¼ê°€ ìˆìœ¼ë©´ System Promptì— ì¶”ê°€
        if analysis_results and len(analysis_results) > 0:
            analysis_context = "\n\n=== í˜„ì¬ ë¶„ì„ëœ ì…ì§€ ì •ë³´ ===\n"

            for i, location in enumerate(analysis_results, 1):
                analysis_context += f"\n[{i}ìˆœìœ„] {location.get('name', 'ì•Œ ìˆ˜ ì—†ìŒ')}\n"
                analysis_context += f"- ì¢…í•© ì ìˆ˜: {location.get('score', 0)}ì \n"

                metrics = location.get('metrics', {})
                analysis_context += f"- ì…ì§€ ì ìˆ˜: {metrics.get('location', 0)}ì \n"
                analysis_context += f"- ìœ ë™ì¸êµ¬ ì ìˆ˜: {metrics.get('footTraffic', 0)}ì \n"
                analysis_context += f"- ì„ëŒ€ë£Œ ì ìˆ˜: {metrics.get('rent', 0)}ì \n"
                analysis_context += f"- ê²½ìŸì—…ì²´ ì ìˆ˜: {metrics.get('competition', 0)}ì \n"

                descriptions = location.get('descriptions', {})
                if descriptions:
                    analysis_context += f"\nìƒì„¸ ë¶„ì„:\n"
                    analysis_context += f"  â€¢ ì…ì§€: {descriptions.get('location', '')}\n"
                    analysis_context += f"  â€¢ ìœ ë™ì¸êµ¬: {descriptions.get('footTraffic', '')}\n"
                    analysis_context += f"  â€¢ ì„ëŒ€ë£Œ: {descriptions.get('rent', '')}\n"
                    analysis_context += f"  â€¢ ê²½ìŸì—…ì²´: {descriptions.get('competition', '')}\n"

            analysis_context += "\nìœ„ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."
            system_prompt = base_prompt + analysis_context
        else:
            system_prompt = base_prompt

        # ============================================
        # ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ ìƒì„± (ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬í•¨)
        # ============================================
        messages_list = [
            {
                "role": "system",
                "content": system_prompt
            }
        ]

        # ëŒ€í™” íˆìŠ¤í† ë¦¬ê°€ ìˆìœ¼ë©´ ì¶”ê°€ (ìµœê·¼ 10ê°œë§Œ)
        if conversation_history and len(conversation_history) > 0:
            messages_list.extend(conversation_history)

        # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        messages_list.append({
            "role": "user",
            "content": user_message
        })

        # Logging: Messages to OpenAI
        print("[Backend] Sending to OpenAI, message count:", len(messages_list))
        print("[Backend] Message structure:")
        for i, msg in enumerate(messages_list, 1):
            role = msg.get('role', 'unknown')
            content_preview = msg.get('content', '')[:50]
            print(f"  {i}. [{role}] {content_preview}...")

        # ============================================
        # OpenAI API í˜¸ì¶œ
        # ============================================
        response = client.chat.completions.create(
            model="gpt-4o-mini",  # ì‚¬ìš©í•  AI ëª¨ë¸
            messages=messages_list,  # ì „ì²´ ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬í•¨
            max_completion_tokens=500,  # ìµœëŒ€ ì‘ë‹µ ê¸¸ì´
            temperature=0.7,        # ì°½ì˜ì„± (0~1, ë†’ì„ìˆ˜ë¡ ì°½ì˜ì )
        )

        # AI ì‘ë‹µ ì¶”ì¶œ
        ai_reply = response.choices[0].message.content

        # Logging: OpenAI response
        print("[OK] [Backend] OpenAI response:", ai_reply[:100] if ai_reply else "None")
        print("="*50 + "\n")

        # ì‘ë‹µ ë°˜í™˜
        return ChatResponse(
            reply=ai_reply,
            message=user_message
        )

    except Exception as e:
        # ì—ëŸ¬ ë°œìƒ ì‹œ ì²˜ë¦¬
        # ì˜ˆ: API í‚¤ ë¬¸ì œ, ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë“±
        print(f"Error: {str(e)}")  # ì„œë²„ ë¡œê·¸ì— ì¶œë ¥

        raise HTTPException(
            status_code=500,
            detail=f"ì±—ë´‡ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


# ============================================
# RAG ì±—ë´‡ ì—”ë“œí¬ì¸íŠ¸ (ì§€ì‹ ê¸°ë°˜ ë‹µë³€)
# ============================================
@app.post("/api/rag-chat", response_model=RAGChatResponse)
async def rag_chat(request: ChatRequest):
    """
    RAG (Retrieval-Augmented Generation) ê¸°ë°˜ ì±—ë´‡

    ì‘ë™ ë°©ì‹:
    1. ì‚¬ìš©ì ì§ˆë¬¸ ë°›ê¸°
    2. ë²¡í„° DBì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ (BGE-M3-KO ì„ë² ë”©)
    3. ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ OpenAI API í˜¸ì¶œ
    4. ì§€ì‹ ê¸°ë°˜ ë‹µë³€ + ì°¸ê³  ë¬¸ì„œ ë°˜í™˜

    ì¥ì :
    - ì—…ë¡œë“œëœ ì§€ì‹ ë¬¸ì„œ ê¸°ë°˜ ì •í™•í•œ ë‹µë³€
    - ì¶œì²˜ ì œê³µ (ì‹ ë¢°ì„±)

    ë‹¨ì :
    - ì¼ë°˜ ì±—ë´‡ë³´ë‹¤ ëŠë¦¼ (2~5ì´ˆ)
    - í† í° ì‚¬ìš©ëŸ‰ ë§ìŒ (ë¹„ìš© ì•½ 10ë°°)
    """
    try:
        user_message = request.message
        conversation_history = request.conversation_history

        # Logging
        print("\n" + "="*50)
        print("[RAG Backend] Received user message:", user_message)
        print("[RAG Backend] History count:", len(conversation_history) if conversation_history else 0)
        print("="*50 + "\n")

        # RAG ì²´ì¸ ê°€ì ¸ì˜¤ê¸° (Lazy Loading)
        rag = await get_rag_chain()

        # RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        result = await rag.run(
            query=user_message,
            conversation_history=conversation_history,
            top_k=3  # ìƒìœ„ 3ê°œ ë¬¸ì„œ ê²€ìƒ‰
        )

        # Logging
        print("[OK] [RAG Backend] RAG answer generated")
        print(f"   - Sources: {len(result.get('sources', []))}")
        print(f"   - Tokens: {result.get('usage', {}).get('total_tokens', 'N/A')}")
        print("="*50 + "\n")

        # ì‘ë‹µ ë°˜í™˜
        return RAGChatResponse(
            reply=result["answer"],
            message=user_message,
            sources=result.get("sources", []),
            usage=result.get("usage")
        )

    except Exception as e:
        print(f"[ERROR] [RAG Backend] Error: {str(e)}")
        print("="*50 + "\n")

        raise HTTPException(
            status_code=500,
            detail=f"RAG ì±—ë´‡ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


# ============================================
# RAG ì±—ë´‡ ìŠ¤íŠ¸ë¦¬ë° ì—”ë“œí¬ì¸íŠ¸
# ============================================

async def stream_rag_response(
    query: str,
    conversation_history: Optional[List[Dict[str, str]]] = None,
    top_k: int = 3
):
    """
    RAG ì‘ë‹µì„ SSE ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì „ì†¡
    """
    import asyncio

    try:
        # RAG ì²´ì¸ ê°€ì ¸ì˜¤ê¸°
        rag = await get_rag_chain()

        # ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ (async for ì‚¬ìš©)
        async for chunk in rag.stream_run(
            query=query,
            conversation_history=conversation_history,
            top_k=top_k
        ):
            # SSE í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ì „ì†¡
            chunk_type = chunk.get("type")
            content = chunk.get("content")

            if chunk_type == "sources":
                # ì°¸ê³  ë¬¸ì„œ ì •ë³´ ì „ì†¡
                yield f"data: {json.dumps({'event': 'sources', 'sources': content})}\n\n"
            elif chunk_type == "web_results":
                # ì›¹ ê²€ìƒ‰ ê²°ê³¼ ì „ì†¡
                yield f"data: {json.dumps({'event': 'web_results', 'web_results': content})}\n\n"
            elif chunk_type == "answer":
                # ë‹µë³€ ì²­í¬ ì „ì†¡ (ASCII ì´ìŠ¤ì¼€ì´í”„ë¡œ ì•ˆì „í•˜ê²Œ ì „ì†¡)
                data = f"data: {json.dumps({'event': 'answer', 'content': content})}\n\n"
                yield data
                # ì¦‰ì‹œ í”ŒëŸ¬ì‹œë¥¼ ìœ„í•´ ì•„ì£¼ ì§§ì€ ëŒ€ê¸°
                await asyncio.sleep(0)
            elif chunk_type == "error":
                # ì—ëŸ¬ ì „ì†¡
                yield f"data: {json.dumps({'event': 'error', 'message': content})}\n\n"

        # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ
        yield f"data: {json.dumps({'event': 'done'})}\n\n"

    except Exception as e:
        error_msg = json.dumps({
            "event": "error",
            "message": f"RAG ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {str(e)}"
        })
        yield f"data: {error_msg}\n\n"


@app.post("/api/rag-chat-stream")
async def rag_chat_stream(request: ChatRequest):
    """
    RAG ì±—ë´‡ ìŠ¤íŠ¸ë¦¬ë° ì—”ë“œí¬ì¸íŠ¸ (SSE)

    ì‘ë™ ë°©ì‹:
    1. ì‚¬ìš©ì ì§ˆë¬¸ ë°›ê¸°
    2. ë²¡í„° DBì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    3. ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ OpenAI API ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
    4. ì‹¤ì‹œê°„ ë‹µë³€ + ì°¸ê³  ë¬¸ì„œ ë°˜í™˜
    """
    print("\n" + "="*50)
    print("[RAG Stream] Received request:")
    print(f"  - query: {request.message[:50]}...")
    print(f"  - history: {len(request.conversation_history) if request.conversation_history else 0} items")
    print("="*50 + "\n")

    return StreamingResponse(
        stream_rag_response(
            query=request.message,
            conversation_history=request.conversation_history,
            top_k=3
        ),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache, no-transform",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
            "Content-Encoding": "none",
            "Transfer-Encoding": "chunked"
        }
    )


# ============================================
# MISO API í”„ë¡ì‹œ ì—”ë“œí¬ì¸íŠ¸ (Streaming)
# ============================================

class MisoChatRequest(BaseModel):
    """
    MISO ì±—ë´‡ ìš”ì²­ í˜•ì‹
    """
    query: str  # ì‚¬ìš©ì ì§ˆë¬¸
    conversation_id: Optional[str] = ""  # ëŒ€í™” ID (ì—°ì† ëŒ€í™”ìš©)
    user: Optional[str] = "user-001"  # ì‚¬ìš©ì ì‹ë³„ì
    inputs: Optional[Dict[str, Any]] = {}  # ì¶”ê°€ ì…ë ¥ ë³€ìˆ˜

# MISO API ì—ëŸ¬ ë©”ì‹œì§€ ë§¤í•‘
MISO_ERROR_MESSAGES = {
    "Conversation does not exists": "ìš”ì²­í•œ ëŒ€í™”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìƒˆ ëŒ€í™”ë¥¼ ì‹œì‘í•´ì£¼ì„¸ìš”.",
    "invalid_param": "ì˜ëª»ëœ íŒŒë¼ë¯¸í„°ê°€ ì…ë ¥ë˜ì—ˆìŠµë‹ˆë‹¤.",
    "app_unavailable": "ì•± ì„¤ì •ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.",
    "provider_not_initialize": "ëª¨ë¸ ì¸ì¦ ì •ë³´ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.",
    "provider_quota_exceeded": "API í˜¸ì¶œ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.",
    "model_currently_not_support": "í˜„ì¬ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
    "completion_request_error": "í…ìŠ¤íŠ¸ ìƒì„± ìš”ì²­ì— ì‹¤íŒ¨í•˜ì˜€ìŠµë‹ˆë‹¤.",
    "internal_server_error": "ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
}

async def stream_miso_response(query: str, conversation_id: str, user: str, inputs: dict):
    """
    MISO API SSE ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ í”„ë¡ì‹œ
    """
    miso_api_key = "app-yZ7SPwZItUQCpmOu3wyxPc0h"

    if not miso_api_key:
        error_msg = json.dumps({
            "event": "error",
            "message": "MISO_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        }, ensure_ascii=False)
        yield f"data: {error_msg}\n\n"
        return

    async with httpx.AsyncClient(timeout=60.0) as client:
        try:
            async with client.stream(
                "POST",
                "https://api.miso.gs/ext/v1/chat",
                headers={
                    "Authorization": f"Bearer {miso_api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "inputs": inputs,
                    "query": query,
                    "mode": "streaming",
                    "conversation_id": conversation_id,
                    "user": user
                }
            ) as response:
                # ì—ëŸ¬ ì‘ë‹µ ì²˜ë¦¬
                if response.status_code != 200:
                    error_body = await response.aread()
                    try:
                        error_data = json.loads(error_body)
                        error_code = error_data.get("code", "unknown")
                        error_detail = MISO_ERROR_MESSAGES.get(
                            error_code,
                            error_data.get("message", "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
                        )
                    except:
                        error_detail = f"HTTP {response.status_code} ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

                    error_msg = json.dumps({
                        "event": "error",
                        "message": error_detail
                    }, ensure_ascii=False)
                    yield f"data: {error_msg}\n\n"
                    return

                # SSE ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì „ë‹¬
                async for line in response.aiter_lines():
                    if line:
                        yield f"{line}\n"
                    else:
                        yield "\n"

        except httpx.TimeoutException:
            error_msg = json.dumps({
                "event": "error",
                "message": "ìš”ì²­ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            }, ensure_ascii=False)
            yield f"data: {error_msg}\n\n"
        except httpx.RequestError as e:
            error_msg = json.dumps({
                "event": "error",
                "message": f"ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            }, ensure_ascii=False)
            yield f"data: {error_msg}\n\n"

@app.post("/api/miso-chat")
async def miso_chat(request: MisoChatRequest):
    """
    MISO API í”„ë¡ì‹œ ì—”ë“œí¬ì¸íŠ¸ (SSE Streaming)

    - í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì´ ì—”ë“œí¬ì¸íŠ¸ë¥¼ í˜¸ì¶œí•˜ë©´
    - MISO APIë¡œ ìš”ì²­ì„ ì „ë‹¬í•˜ê³ 
    - SSE ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ê·¸ëŒ€ë¡œ ì „ë‹¬
    """
    print("\n" + "="*50)
    print("[MISO Proxy] Received request:")
    print(f"  - query: {request.query[:50]}...")
    print(f"  - conversation_id: {request.conversation_id}")
    print(f"  - user: {request.user}")
    print("="*50 + "\n")

    return StreamingResponse(
        stream_miso_response(
            query=request.query,
            conversation_id=request.conversation_id or "",
            user=request.user or "user-001",
            inputs=request.inputs or {}
        ),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )

# ============================================
# ì„œë²„ ì‹¤í–‰ ë°©ë²•
# ============================================
# cd backend
# uvicorn main:app --reload --port 8000
#
# ë˜ëŠ” ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰:
# nohup uvicorn main:app --port 8000 &

# ============================================
# ì½”ë“œ ì„¤ëª… ìš”ì•½
# ============================================
# 1. load_dotenv() - .env íŒŒì¼ì—ì„œ API í‚¤ ë¡œë“œ
# 2. OpenAI(api_key=...) - OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
# 3. client.chat.completions.create() - AIì—ê²Œ ë©”ì‹œì§€ ë³´ë‚´ê¸°
#    - model: ì‚¬ìš©í•  AI ëª¨ë¸ (gpt-3.5-turbo, gpt-4 ë“±)
#    - messages: ëŒ€í™” ë‚´ìš©
#      - system: AIì˜ ì—­í• /ì„±ê²© ì •ì˜
#      - user: ì‚¬ìš©ì ë©”ì‹œì§€
#    - max_tokens: ìµœëŒ€ ì‘ë‹µ ê¸¸ì´
#    - temperature: ì°½ì˜ì„± (0=ë³´ìˆ˜ì , 1=ì°½ì˜ì )
# 4. response.choices[0].message.content - AI ì‘ë‹µ ì¶”ì¶œ
